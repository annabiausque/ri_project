{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Search Retrieval Augmented Generation\n",
    "\n",
    "In this notebook you will implement the following steps:\n",
    "\n",
    "- **Answer selection + evaluation**: Implement a *search-based* conversation framework evaluation framework to evaluate conversation topics made up of conversation turns.\n",
    "- **Answer ranking**: Implement a *re-ranking method* to sort the initial search results. Evaluate the re-ranked results.\n",
    "- **Conversation memory**: Implement a conversational context modeling method to keep track of the conversation state. \n",
    "\n",
    "Submission dates:\n",
    "- **20 October**: first stage retrieval + conversation memory + evaluation\n",
    "- **15 November**: re-ranking with LLM + evaluation\n",
    "- **15 December**: answer generation + evaluation\n",
    "\n",
    "## Test bed and conversation topics\n",
    "The TREC CAST corpus (http://www.treccast.ai/) for Conversational Search is indexed in this cluster and available to be searched behind an OpenSearch API.\n",
    "\n",
    "The queries and the relevance judgments are available through class `ConvSearchEvaluation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX SETTINGS\n",
      "{'kwiz': {'settings': {'index': {'creation_date': '1728153198145',\n",
      "                                 'knn': 'true',\n",
      "                                 'number_of_replicas': '0',\n",
      "                                 'number_of_shards': '1',\n",
      "                                 'provided_name': 'kwiz',\n",
      "                                 'refresh_interval': '-1',\n",
      "                                 'similarity': {'default': {'lambda': '0.7',\n",
      "                                                            'type': 'LMJelinekMercer'}},\n",
      "                                 'uuid': 'qkpQ7pcwS7iT1IOTsfwRNg',\n",
      "                                 'version': {'created': '135238227'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX MAPPINGS\n",
      "{'kwiz': {'mappings': {'properties': {'collection': {'type': 'keyword'},\n",
      "                                      'contents': {'index_options': 'freqs',\n",
      "                                                   'similarity': 'BM25',\n",
      "                                                   'store': True,\n",
      "                                                   'term_vector': 'yes',\n",
      "                                                   'type': 'text'},\n",
      "                                      'doc': {'type': 'keyword'},\n",
      "                                      'sentence_embedding': {'model_id': 'model_kwiz',\n",
      "                                                             'type': 'knn_vector'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX #DOCs\n",
      "{'count': 23596, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n"
     ]
    }
   ],
   "source": [
    "import TRECCASTeval as trec\n",
    "import numpy as np\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import OpenSearchSimpleAPI as osearch\n",
    "import bm25s\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "test_bed = trec.ConvSearchEvaluation()\n",
    "\n",
    "chosen_topic= 77\n",
    "conversation = []\n",
    "topics = {}\n",
    "for topic in test_bed.train_topics:\n",
    "    conv_id = topic['number']\n",
    "    if conv_id not in (1, 2, 4, 7, 15, 17, 18, 22, 23, 24, 25, 27, 30):\n",
    "        continue\n",
    "    for turn in topic['turn']:\n",
    "        turn_id = turn['number']\n",
    "        utterance = turn['raw_utterance']\n",
    "        topic_turn_id = '%d_%d' % (conv_id, turn_id)\n",
    "        topics[topic_turn_id] = utterance\n",
    "        if conv_id == chosen_topic :\n",
    "            conversation.append({\"conv_id\" : conv_id, \"turn_id\" : turn_id, \"utterance\" : utterance})\n",
    "\n",
    "\n",
    "for topic in test_bed.test_topics:\n",
    "    conv_id = topic['number']\n",
    "\n",
    "    if conv_id not in (31, 32, 33, 34, 37, 40, 49, 50, 54, 56, 58, 59, 61, 67, 68, 69, 75, 77, 78, 79):\n",
    "        continue\n",
    "    for turn in topic['turn']:\n",
    "        turn_id = turn['number']\n",
    "        utterance = turn['raw_utterance']\n",
    "        topic_turn_id = '%d_%d' % (conv_id, turn_id)\n",
    "        topics[topic_turn_id] = utterance\n",
    "        if conv_id == chosen_topic :\n",
    "            conversation.append({\"conv_id\" : conv_id, \"turn_id\" : turn_id, \"utterance\" : utterance})\n",
    "\n",
    "\n",
    "opensearch = osearch.OSsimpleAPI()\n",
    "\n",
    "numdocs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  turn                                           query  \\\n",
      "0    1    What's the difference between soup and stew?   \n",
      "1    2                               Is chilli a stew?   \n",
      "2    3                              How about goulash?   \n",
      "3    4                What are popular ones in France?   \n",
      "4    5                          How is cassoulet made?   \n",
      "5    6    Tell me about feijoada and its significance.   \n",
      "6    7  How is it similar or different from cassoulet?   \n",
      "7    8                          Tell about Bigos stew.   \n",
      "8    9                            Why is it important?   \n",
      "9   10              What is the history of Irish stew?   \n",
      "\n",
      "                                   expanded_query  \\\n",
      "0    What's the difference between soup and stew?   \n",
      "1                               Is chilli a stew?   \n",
      "2                              How about goulash?   \n",
      "3                What are popular ones in France?   \n",
      "4                          How is cassoulet made?   \n",
      "5    Tell me about feijoada and its significance.   \n",
      "6  How is it similar or different from cassoulet?   \n",
      "7                          Tell about Bigos stew.   \n",
      "8                            Why is it important?   \n",
      "9              What is the history of Irish stew?   \n",
      "\n",
      "                                        top passages  \\\n",
      "0  [What's the difference between a soup and a st...   \n",
      "1  [Fricasse â tiny sandwich with tuna, harissa...   \n",
      "2  [How to cook the perfect goulash. Goulash is a...   \n",
      "3  [What are the most popular sports in France? C...   \n",
      "4  [The region once known as the province of Lang...   \n",
      "5  [I want to do the Company Secretary course fro...   \n",
      "6  [The phrases hard to lose electrons and easy t...   \n",
      "7  [A national dish of Poland, bigos is a traditi...   \n",
      "8  [GDP per capita is viewed to be important as i...   \n",
      "9  [Irish Beef Stew [slow cooker]. [Slow cooker v...   \n",
      "\n",
      "                                                 _id  \n",
      "0      [MARCO_3831856, MARCO_3712923, MARCO_3070255]  \n",
      "1      [MARCO_2801272, MARCO_3318351, MARCO_3318357]  \n",
      "2  [MARCO_1595926, CAR_60759f5845c44489d3f277c92f...  \n",
      "3  [MARCO_3581032, CAR_4c169e88f8453e8dea2b89d9f1...  \n",
      "4  [CAR_66eff27699239eb298881ab9f16759580ff73208,...  \n",
      "5      [MARCO_6083641, MARCO_8827796, MARCO_8344507]  \n",
      "6  [MARCO_6005525, CAR_66eff27699239eb298881ab9f1...  \n",
      "7        [MARCO_228565, MARCO_228568, MARCO_4418424]  \n",
      "8       [MARCO_673867, MARCO_5028938, MARCO_3823042]  \n",
      "9       [MARCO_571975, MARCO_7907228, MARCO_3712925]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def BM_retrieval(k):\n",
    "    #metrics_df = pd.DataFrame(columns=['turn', 'query', '_id'])\n",
    "    BM25data = []\n",
    "    bm25_doc_ids = []\n",
    "    for element in conversation :\n",
    "        topic = str(chosen_topic)\n",
    "        turn = str(element['turn_id'])\n",
    "        utterance = topic + '_' + turn\n",
    "   \n",
    "        query = topics[utterance]\n",
    "        opensearch_results = opensearch.search_body(query, numDocs = k)\n",
    "        best_docs = []\n",
    "        best_passages = []\n",
    "        content_to_id = {}  \n",
    "        for index, row in opensearch_results.iterrows():\n",
    "            doc_id = row['_id']\n",
    "            doc_body = opensearch.get_doc_body(doc_id)\n",
    "            #new_row = {'turn': utterance, 'query': element['utterance'], '_id': doc_id}\n",
    "            #metrics_df = pd.concat([metrics_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "            best_passages.append(doc_body)\n",
    "            best_docs.append(doc_id)\n",
    "            content_to_id[doc_body] = doc_id  \n",
    "        bm25_doc_ids.append([content_to_id[doc] for doc in best_passages if doc in content_to_id])\n",
    "   \n",
    "            \n",
    "        BM25data.append({'turn': turn, 'query': element[\"utterance\"], \"expanded_query\" : query,  'top passages': best_passages, '_id': best_docs})\n",
    "\n",
    "    df = pd.DataFrame(BM25data)\n",
    "   \n",
    "    return df\n",
    "\n",
    "print(BM_retrieval(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  turn                                           query  \\\n",
      "0    1    What's the difference between soup and stew?   \n",
      "1    2                               Is chilli a stew?   \n",
      "2    3                              How about goulash?   \n",
      "3    4                What are popular ones in France?   \n",
      "4    5                          How is cassoulet made?   \n",
      "5    6    Tell me about feijoada and its significance.   \n",
      "6    7  How is it similar or different from cassoulet?   \n",
      "7    8                          Tell about Bigos stew.   \n",
      "8    9                            Why is it important?   \n",
      "9   10              What is the history of Irish stew?   \n",
      "\n",
      "                                                 _id  \n",
      "0  [MARCO_16539, MARCO_7312785, MARCO_4009632, MA...  \n",
      "1  [CAR_e0559ce4c6079c402ca8c187105fc3099decb965,...  \n",
      "2  [CAR_33deda658ce5f81b4ffca6fd9be8efbc60436928,...  \n",
      "3  [CAR_e7d160e7a8dde88710838eb8dd6dfc7898a02d4f,...  \n",
      "4  [CAR_c44cfd57976b8ef0ee9f5d1b3446d7b9e3106c84,...  \n",
      "5  [CAR_5a75fe18050cc4ebdb9a03075eb4e72032f4c96b,...  \n",
      "6  [CAR_c44cfd57976b8ef0ee9f5d1b3446d7b9e3106c84,...  \n",
      "7  [CAR_e0559ce4c6079c402ca8c187105fc3099decb965,...  \n",
      "8  [MARCO_6806330, MARCO_3446096, MARCO_3354906, ...  \n",
      "9  [CAR_36d2c4f1e6cf2e5a1d7438fe74daac0ed47932d7,...  \n"
     ]
    }
   ],
   "source": [
    "# LMDRetriever\n",
    "class LMDRetriever:\n",
    "    def __init__(self, opensearch, corpus_ids):\n",
    "        self.opensearch = opensearch\n",
    "        self.corpus_ids = corpus_ids\n",
    "        self.corpus_length = 0\n",
    "        self.doc_count = len(corpus_ids)\n",
    "        self.index = self.build_index()\n",
    "        self.collection_frequency = self.build_collection_frequency()\n",
    "        self.mu = self.calculate_mu()\n",
    "    \n",
    "    def calculate_mu(self):\n",
    "        avg_doc_length = self.corpus_length / self.doc_count\n",
    "        return 0.1 * avg_doc_length\n",
    "\n",
    "    def build_index(self):\n",
    "        index = {}\n",
    "        for doc_id in self.corpus_ids:\n",
    "            term_vectors = self.opensearch.doc_term_vectors(doc_id)\n",
    "            if term_vectors:\n",
    "                terms = term_vectors[3]\n",
    "                for term, stats in terms.items():\n",
    "                    if term not in index:\n",
    "                        index[term] = {}\n",
    "                    index[term][doc_id] = stats[0]\n",
    "                    self.corpus_length += stats[0]\n",
    "        return index\n",
    "\n",
    "    def build_collection_frequency(self):\n",
    "        collection_frequency = {}\n",
    "        for doc_id in self.corpus_ids:\n",
    "            term_vectors = self.opensearch.doc_term_vectors(doc_id)\n",
    "            if term_vectors:\n",
    "                terms = term_vectors[3]\n",
    "                for term, stats in terms.items():\n",
    "                    if term not in collection_frequency:\n",
    "                        collection_frequency[term] = 0\n",
    "                    collection_frequency[term] += stats[2]\n",
    "        return collection_frequency\n",
    "\n",
    "    def score(self, query, doc_id):\n",
    "        score = 1.0\n",
    "        term_vectors = self.opensearch.doc_term_vectors(doc_id)\n",
    "        if term_vectors:\n",
    "            terms = term_vectors[3]\n",
    "            doc_length = sum([stats[0] for stats in terms.values()])\n",
    "            for term in query.split():\n",
    "                tf = terms.get(term, [0])[0]\n",
    "                cf = self.collection_frequency.get(term, 0)\n",
    "                p_ml = cf / self.corpus_length\n",
    "                p_lmd = (tf + self.mu * p_ml) / (doc_length + self.mu)\n",
    "                if p_lmd > 0:\n",
    "                    score *= p_lmd\n",
    "        return score\n",
    "\n",
    "    def retrieve(self, query, k):\n",
    "        scores = []\n",
    "        for doc_id in self.corpus_ids:\n",
    "            score = self.score(query, doc_id)\n",
    "            scores.append((doc_id, score))\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return scores[:k]\n",
    "\n",
    "bm25_results = BM_retrieval(100)\n",
    "\n",
    "#print(bm25_results)\n",
    "\n",
    "#print('-------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "reranked_df = pd.DataFrame(columns=[\"turn\", \"query\", \"_id\"])\n",
    "\n",
    "\n",
    "for index, row in bm25_results.iterrows():\n",
    "   \n",
    "    bm25_doc_ids = row[\"_id\"] \n",
    "    turn = row[\"turn\"]\n",
    "    query = row[\"expanded_query\"]\n",
    "\n",
    "    # Créer une instance de LMDRetriever avec les doc_ids récupérés\n",
    "    lmd_retriever = LMDRetriever(opensearch=opensearch, corpus_ids=bm25_doc_ids)\n",
    "\n",
    "    # Reranking avec la méthode retrieve\n",
    "    reranked_results = lmd_retriever.retrieve(query, k=100)\n",
    "\n",
    "    # Extraire les passages du reranking\n",
    "    top_N_passages = [doc_id for doc_id, score in reranked_results]\n",
    "\n",
    "    # Créer un DataFrame temporaire pour la nouvelle ligne\n",
    "    new_row = pd.DataFrame({\n",
    "        \"turn\": [turn],\n",
    "        \"query\": [query],\n",
    "        \"_id\": [top_N_passages]\n",
    "    })\n",
    "\n",
    "\n",
    "    # Utiliser pd.concat pour ajouter la nouvelle ligne au DataFrame final\n",
    "    reranked_df = pd.concat([reranked_df, new_row], ignore_index=True)\n",
    "\n",
    "# Afficher le DataFrame final\n",
    "print(reranked_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMD\n",
      "Turn: 77_1\n",
      "P@10: 0.6, Recall: 0.6153846153846154, AP: 0.21890349531082884, NDCG@5: 0.1599902354470432\n",
      "\n",
      "Turn: 77_2\n",
      "P@10: 0.0, Recall: 0.4, AP: 0.009639830508474575, NDCG@5: 0.0\n",
      "\n",
      "Turn: 77_3\n",
      "P@10: 0.3, Recall: 0.7435897435897436, AP: 0.2410340073882552, NDCG@5: 0.10157051007554901\n",
      "\n",
      "Turn: 77_4\n",
      "P@10: 0.1, Recall: 0.20833333333333334, AP: 0.02203193303734692, NDCG@5: 0.25326335410655304\n",
      "\n",
      "Turn: 77_5\n",
      "P@10: 0.6, Recall: 0.9444444444444444, AP: 0.4116411106384749, NDCG@5: 0.25064969555809496\n",
      "\n",
      "Turn: 77_6\n",
      "P@10: 0.2, Recall: 0.125, AP: 0.03187957875457875, NDCG@5: 0.18322608909137006\n",
      "\n",
      "Turn: 77_7\n",
      "P@10: 0.2, Recall: 0.22857142857142856, AP: 0.03654396991052007, NDCG@5: 0.10907458963273321\n",
      "\n",
      "Turn: 77_8\n",
      "P@10: 0.2, Recall: 0.84375, AP: 0.20964821424602653, NDCG@5: 0.0\n",
      "\n",
      "Turn: 77_9\n",
      "P@10: 0, Recall: 0, AP: 0, NDCG@5: 0\n",
      "\n",
      "Turn: 77_10\n",
      "P@10: 0, Recall: 0, AP: 0, NDCG@5: 0\n",
      "\n",
      "['77_1', '77_2', '77_3', '77_4', '77_5', '77_6', '77_7', '77_8', '77_9', '77_10'] [0.21890349531082884, 0.009639830508474575, 0.2410340073882552, 0.02203193303734692, 0.4116411106384749, 0.03187957875457875, 0.03654396991052007, 0.20964821424602653, 0, 0] [0.1599902354470432, 0.0, 0.10157051007554901, 0.25326335410655304, 0.25064969555809496, 0.18322608909137006, 0.10907458963273321, 0.0, 0, 0] [0.6, 0.0, 0.3, 0.1, 0.6, 0.2, 0.2, 0.2, 0, 0] [0.6153846153846154, 0.4, 0.7435897435897436, 0.20833333333333334, 0.9444444444444444, 0.125, 0.22857142857142856, 0.84375, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "turns = []\n",
    "LMD_ap_values = []\n",
    "LMD_ndcg_values = []\n",
    "LMD_precision_values = []\n",
    "LMD_recall_values = []\n",
    "\n",
    "print(\"LMD\")\n",
    "\n",
    "for index, row in reranked_df.iterrows():\n",
    "    try:\n",
    "    \n",
    "        turn = f\"77_{row['turn']}\"  \n",
    "        query = row['query']\n",
    "        docs = row['_id']  \n",
    "\n",
    "        result_df = pd.DataFrame({\"_id\": docs})\n",
    "\n",
    "        p10, recall, ap, ndcg5 = test_bed.eval(result_df, turn)\n",
    "        turns.append(turn)\n",
    "        LMD_ap_values.append(ap)\n",
    "        LMD_ndcg_values.append(ndcg5)\n",
    "        LMD_precision_values.append(p10)\n",
    "        LMD_recall_values.append(recall)\n",
    "\n",
    "        print(f\"Turn: {turn}\")\n",
    "        \n",
    "    \n",
    "        print(f\"P@10: {p10}, Recall: {recall}, AP: {ap}, NDCG@5: {ndcg5}\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "  \n",
    "        print(f\"Erreur sur le tour {turn}: {e}\")\n",
    "        break  \n",
    "\n",
    "print(turns,LMD_ap_values,LMD_ndcg_values,LMD_precision_values, LMD_recall_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Supprimer les tours avec des scores de 0 pour éviter les erreurs\n",
    "filtered_turns = []\n",
    "filtered_ap_values = []\n",
    "filtered_ndcg_values = []\n",
    "filtered_precision_values = []\n",
    "filtered_recall_values = []\n",
    "\n",
    "for i in range(len(turns)):\n",
    "    # Vérifier si AP ou NDCG est nul\n",
    "    if LMD_ap_values[i] != 0 or LMD_ndcg_values[i] != 0:\n",
    "        filtered_turns.append(turns[i])\n",
    "        filtered_ap_values.append(LMD_ap_values[i])\n",
    "        filtered_ndcg_values.append(LMD_ndcg_values[i])\n",
    "\n",
    "    # Vérifier si Précision ou Rappel est nul (pour éviter d'afficher un point isolé)\n",
    "    if LMD_precision_values[i] != 0 or LMD_recall_values[i] != 0:\n",
    "        filtered_precision_values.append(LMD_precision_values[i])\n",
    "        filtered_recall_values.append(LMD_recall_values[i])\n",
    "\n",
    "# Activer le mode interactif pour améliorer la réactivité\n",
    "plt.ion()\n",
    "\n",
    "# 1. Évolution de l'AP et du NDCG au fil des tours de conversation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(filtered_turns, filtered_ap_values, marker='o', label='AP', linestyle='-')\n",
    "plt.plot(filtered_turns, filtered_ndcg_values, marker='x', label='NDCG@5', linestyle='-')\n",
    "plt.title(\"Évolution de l'AP et du NDCG@5 au fil des tours de conversation\")\n",
    "plt.xlabel(\"Tours de conversation\")\n",
    "plt.ylabel(\"Scores\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(filtered_recall_values, filtered_precision_values, marker='o', label='Precision-Recall')\n",
    "plt.title(\"Precision-Recall\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
