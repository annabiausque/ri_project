{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Search Retrieval Augmented Generation\n",
    "\n",
    "In this notebook you will implement the following steps:\n",
    "\n",
    "- **Answer selection + evaluation**: Implement a *search-based* conversation framework evaluation framework to evaluate conversation topics made up of conversation turns.\n",
    "- **Answer ranking**: Implement a *re-ranking method* to sort the initial search results. Evaluate the re-ranked results.\n",
    "- **Conversation memory**: Implement a conversational context modeling method to keep track of the conversation state. \n",
    "\n",
    "Submission dates:\n",
    "- **20 October**: first stage retrieval + conversation memory + evaluation\n",
    "- **15 November**: re-ranking with LLM + evaluation\n",
    "- **15 December**: answer generation + evaluation\n",
    "\n",
    "## Test bed and conversation topics\n",
    "The TREC CAST corpus (http://www.treccast.ai/) for Conversational Search is indexed in this cluster and available to be searched behind an OpenSearch API.\n",
    "\n",
    "The queries and the relevance judgments are available through class `ConvSearchEvaluation`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anna/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acknowledged': True, 'shards_acknowledged': True}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX SETTINGS\n",
      "{'kwiz': {'settings': {'index': {'creation_date': '1728153198145',\n",
      "                                 'knn': 'true',\n",
      "                                 'number_of_replicas': '0',\n",
      "                                 'number_of_shards': '1',\n",
      "                                 'provided_name': 'kwiz',\n",
      "                                 'refresh_interval': '-1',\n",
      "                                 'similarity': {'default': {'lambda': '0.7',\n",
      "                                                            'type': 'LMJelinekMercer'}},\n",
      "                                 'uuid': 'qkpQ7pcwS7iT1IOTsfwRNg',\n",
      "                                 'version': {'created': '135238227'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX MAPPINGS\n",
      "{'kwiz': {'mappings': {'properties': {'collection': {'type': 'keyword'},\n",
      "                                      'contents': {'index_options': 'freqs',\n",
      "                                                   'similarity': 'BM25',\n",
      "                                                   'store': True,\n",
      "                                                   'term_vector': 'yes',\n",
      "                                                   'type': 'text'},\n",
      "                                      'doc': {'type': 'keyword'},\n",
      "                                      'sentence_embedding': {'model_id': 'model_kwiz',\n",
      "                                                             'type': 'knn_vector'}}}}}\n",
      "\n",
      "----------------------------------------------------------------------------------- INDEX #DOCs\n",
      "{'count': 23596, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}}\n"
     ]
    }
   ],
   "source": [
    "import TRECCASTeval as trec\n",
    "import numpy as np\n",
    "import pprint\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import OpenSearchSimpleAPI as osearch\n",
    "import bm25s\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "test_bed = trec.ConvSearchEvaluation()\n",
    "\n",
    "\n",
    "topics = {}\n",
    "for topic in test_bed.train_topics:\n",
    "    conv_id = topic['number']\n",
    "    if conv_id not in (1, 2, 4, 7, 15, 17, 18, 22, 23, 24, 25, 27, 30):\n",
    "        continue\n",
    "    for turn in topic['turn']:\n",
    "        turn_id = turn['number']\n",
    "        utterance = turn['raw_utterance']\n",
    "        topic_turn_id = '%d_%d' % (conv_id, turn_id)\n",
    "        topics[topic_turn_id] = utterance\n",
    "\n",
    "for topic in test_bed.test_topics:\n",
    "    conv_id = topic['number']\n",
    "    if conv_id not in (31, 32, 33, 34, 37, 40, 49, 50, 54, 56, 58, 59, 61, 67, 68, 69, 75, 77, 78, 79):\n",
    "        continue\n",
    "    for turn in topic['turn']:\n",
    "        turn_id = turn['number']\n",
    "        utterance = turn['raw_utterance']\n",
    "        topic_turn_id = '%d_%d' % (conv_id, turn_id)\n",
    "        topics[topic_turn_id] = utterance\n",
    "\n",
    "\n",
    "opensearch = osearch.OSsimpleAPI()\n",
    "\n",
    "numdocs = 100\n",
    "test_query = topics['77_1']\n",
    "\n",
    "\n",
    "opensearch_results = opensearch.search_body(test_query, numDocs=numdocs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unprocessed query: What's the difference between soup and stew?\n",
      "Processed: what differ soup stew\n",
      "Expanded Query: what differ soup stew\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "def expand_query(query):\n",
    "    expanded_query = query \n",
    "    return expanded_query\n",
    "\n",
    "\n",
    "query = test_query\n",
    "processed_query = preprocess_text(query)\n",
    "expanded_query = expand_query(processed_query)\n",
    "print('Unprocessed query: ' + query)\n",
    "print('Processed: ' + processed_query)\n",
    "print('Expanded Query:', expanded_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1e76c1545e43d38b26b54351037977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Query: Tokenized(ids=[[0, 1, 2, 3]], vocab={'what': 0, 'differ': 1, 'soup': 2, 'stew': 3})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "232a5d5205d94d2791d764725b723af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split strings:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b696d63f0da8433e87c6f33cf7ac59b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Count Tokens:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40989a7ce5a49c49447bed23f72d08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Compute Scores:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6718fdb8d64f409d35540400735555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BM25S Retrieve:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "corpus = []\n",
    "doc_ids = []\n",
    "content_to_id = {}  \n",
    "for index, row in opensearch_results.iterrows():\n",
    "    doc_id = row['_id']\n",
    "    doc_body = opensearch.get_doc_body(doc_id)\n",
    "    corpus.append(doc_body)\n",
    "    doc_ids.append(doc_id)\n",
    "    content_to_id[doc_body] = doc_id  \n",
    "\n",
    "# BM25 retrieval\n",
    "tokenized_query = bm25s.tokenize(expanded_query)\n",
    "print(\"Tokenized Query:\", tokenized_query)\n",
    "\n",
    "\n",
    "retriever = bm25s.BM25(corpus=corpus)\n",
    "retriever.index(bm25s.tokenize(corpus))\n",
    "\n",
    "k = 100\n",
    "bm25_results, bm25_scores = retriever.retrieve(tokenized_query, k=k)\n",
    "\n",
    "\n",
    "bm25_doc_ids = [content_to_id[doc] for doc in bm25_results[0] if doc in content_to_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LMDRetriever\n",
    "class LMDRetriever:\n",
    "    def __init__(test_bed, opensearch, corpus_ids):\n",
    "        test_bed.opensearch = opensearch\n",
    "        test_bed.corpus_ids = corpus_ids\n",
    "        test_bed.corpus_length = 0\n",
    "        test_bed.doc_count = len(corpus_ids)\n",
    "        test_bed.index = test_bed.build_index()\n",
    "        test_bed.collection_frequency = test_bed.build_collection_frequency()\n",
    "        test_bed.mu = test_bed.calculate_mu()\n",
    "    \n",
    "    def calculate_mu(test_bed):\n",
    "        avg_doc_length = test_bed.corpus_length / test_bed.doc_count\n",
    "        return 0.1 * avg_doc_length\n",
    "\n",
    "    def build_index(test_bed):\n",
    "        index = {}\n",
    "        for doc_id in test_bed.corpus_ids:\n",
    "            term_vectors = test_bed.opensearch.doc_term_vectors(doc_id)\n",
    "            if term_vectors:\n",
    "                terms = term_vectors[3]\n",
    "                for term, stats in terms.items():\n",
    "                    if term not in index:\n",
    "                        index[term] = {}\n",
    "                    index[term][doc_id] = stats[0]\n",
    "                    test_bed.corpus_length += stats[0]\n",
    "        return index\n",
    "\n",
    "    def build_collection_frequency(test_bed):\n",
    "        collection_frequency = {}\n",
    "        for doc_id in test_bed.corpus_ids:\n",
    "            term_vectors = test_bed.opensearch.doc_term_vectors(doc_id)\n",
    "            if term_vectors:\n",
    "                terms = term_vectors[3]\n",
    "                for term, stats in terms.items():\n",
    "                    if term not in collection_frequency:\n",
    "                        collection_frequency[term] = 0\n",
    "                    collection_frequency[term] += stats[2]\n",
    "        return collection_frequency\n",
    "\n",
    "    def score(test_bed, query, doc_id):\n",
    "        score = 1.0\n",
    "        term_vectors = test_bed.opensearch.doc_term_vectors(doc_id)\n",
    "        if term_vectors:\n",
    "            terms = term_vectors[3]\n",
    "            doc_length = sum([stats[0] for stats in terms.values()])\n",
    "            for term in query.split():\n",
    "                tf = terms.get(term, [0])[0]\n",
    "                cf = test_bed.collection_frequency.get(term, 0)\n",
    "                p_ml = cf / test_bed.corpus_length\n",
    "                p_lmd = (tf + test_bed.mu * p_ml) / (doc_length + test_bed.mu)\n",
    "                if p_lmd > 0:\n",
    "                    score *= p_lmd\n",
    "        return score\n",
    "\n",
    "    def retrieve(test_bed, query, k):\n",
    "        scores = []\n",
    "        for doc_id in test_bed.corpus_ids:\n",
    "            score = test_bed.score(query, doc_id)\n",
    "            scores.append((doc_id, score))\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "        return scores[:k]\n",
    "    \n",
    "# LMDRetriever to rerank the BM25 results\n",
    "lmd_retriever = LMDRetriever(opensearch=opensearch, corpus_ids=bm25_doc_ids)\n",
    "reranked_results = lmd_retriever.retrieve(expanded_query, k=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   turn                                         query  \\\n",
      "0  77_1  What's the difference between soup and stew?   \n",
      "\n",
      "                                                 _id  \n",
      "0  [MARCO_16539, MARCO_7312785, MARCO_4009632, MA...  \n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "best_docs = []\n",
    "for rank, (doc_id, score) in enumerate(reranked_results, start=1):\n",
    "    doc_content = opensearch.get_doc_body(doc_id)\n",
    "    best_docs.append(str(doc_id))\n",
    "    #print(f\"Rank {rank} (score: {score:.6f}):\\n{doc_content}\\n\")\n",
    "\n",
    "result_df = pd.DataFrame({\"turn\" : \"77_1\", \"query\" : query, '_id' : best_docs })\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p10, recall, ap, ndcg5 \u001b[38;5;241m=\u001b[39m test_bed\u001b[38;5;241m.\u001b[39meval(result_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m77_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(p10,recall, ap, ndcg5 )\n",
      "File \u001b[0;32m~/Desktop/ri_project/TRECCASTeval.py:65\u001b[0m, in \u001b[0;36mConvSearchEvaluation.eval\u001b[0;34m(self, result, topic_turn_id)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# P@10\u001b[39;00m\n\u001b[1;32m     64\u001b[0m top10 \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m---> 65\u001b[0m true_pos\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mintersect1d(top10,query_rel_docs)\n\u001b[1;32m     66\u001b[0m p10 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msize(true_pos) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     68\u001b[0m true_pos\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mintersect1d(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m],query_rel_docs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/lib/arraysetops.py:455\u001b[0m, in \u001b[0;36mintersect1d\u001b[0;34m(ar1, ar2, assume_unique, return_indices)\u001b[0m\n\u001b[1;32m    453\u001b[0m     aux \u001b[38;5;241m=\u001b[39m aux[aux_sort_indices]\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 455\u001b[0m     aux\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    457\u001b[0m mask \u001b[38;5;241m=\u001b[39m aux[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m aux[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    458\u001b[0m int1d \u001b[38;5;241m=\u001b[39m aux[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][mask]\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'list'"
     ]
    }
   ],
   "source": [
    "p10, recall, ap, ndcg5 = test_bed.eval(result_df, '77_1')\n",
    "\n",
    "print(p10,recall, ap, ndcg5 )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
